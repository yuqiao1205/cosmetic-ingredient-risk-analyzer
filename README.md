# ğŸ§´ Cosmetic Ingredient Risk Analyzer

The Cosmetic Ingredient Risk Analyzer helps consumers and researchers evaluate the safety of cosmetic and skincare products by analyzing ingredient lists.  
It supports **image uploads**, **product URLs**, and **manual pasted ingredient lists**, and provides a detailed risk breakdown of each ingredient using a local knowledge base (**LlamaIndex + ChromaDB + Ollama LLM**).

---

## ğŸš€ Features
- ğŸ“¸ **Image Upload**: Upload a photo of a productâ€™s ingredient list â†’ OCR extracts text â†’ Ingredients are analyzed.
- ğŸ”— **URL Input**: Paste a product page link (e.g., Sephora, Clinique) â†’ Automatically scrape and analyze ingredients.
- ğŸ“‹ **Manual Input**: Paste ingredient lists directly for instant analysis.
- ğŸ§  **Smart Extraction**: Local LLM extracts only ingredients, filtering out non-ingredient text.
- ğŸ· **Product Scoring**: Rates the overall safety of the product.
- ğŸ“Š **Risk Scoring**: Ingredients categorized into:
  - **High Risk** (e.g., parabens, talc, BHT)
  - **Medium Risk** (e.g., aluminum salts, PEGs)
  - **Low Risk / Safe** (commonly used ingredients)
- ğŸ“ **Explanations**: Each ingredient includes **risk level** + **impact** (e.g., irritation, toxicity, bioaccumulation).
- ğŸ”’ **Local & Private**: Uses **LlamaIndex + ChromaDB + Ollama** for local processing. No external API calls required.

---

## ğŸ›  Tech Stack

- **Backend**: Python
- **Frontend**: Gradio

- **OCR**: Tesseract (via pytesseract) 

- **LLM Integration**: LlamaIndex + Ollama (local models)

- **Vector Database**: ChromaDB

---

## ğŸ›  Installation & Setup

### 1. Clone the Repository
```bash
git clone https://github.com/yuqiao1205/cosmetic-ingredient-risk-analyzer.git
cd cosmetic-ingredient-risk-analyzer
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate   # On Mac/Linux
venv\Scripts\activate      # On Windows
```

### 3. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 4. Install Ollama
Ollama is required to run the local LLM.

- **Mac/Linux**:  
  [Download & Install Ollama](https://ollama.ai/download)

- **Windows**:  
  Follow setup instructions from [Ollama for Windows](https://ollama.ai/download/windows)

### 5. Pull Local LLM (Gemma 3)
After installing Ollama, pull the **Gemma 3** model (or another supported LLM):
```bash
ollama pull gemma3:4b
```

### 6. Start the Local LLM
Before running the app, make sure Ollama is running:
```bash
ollama run gemma3:4b
```

### 7. Run the App
In another terminal, start the app:
```bash
python3 app.py
```

---

## ğŸ§ª Example Use Cases
- ğŸ“¸ Upload a **photo of Clinique foundation** â†’ Extracts and rates all ingredients.
- ğŸ”— Paste a **Sephora product URL** â†’ Scrapes and analyzes the ingredient list.
- ğŸ“‹ Copy-paste **ingredients directly** â†’ Instantly get a breakdown of risks.
- ğŸ“Š Get categorized reports (e.g., parabens = medium risk, talc = high risk).

---

âœ… Now your **Ingredient Risk Analyzer** runs fully offline with local OCR, LlamaIndex, ChromaDB, and Ollama-powered LLM.

## ğŸ“Œ Note
1. Please make sure that Tesseract is correctly installed on your system. You can check by running tesseract --version in the terminal. If itâ€™s not installed, use:

brew install tesseract

2. Before running the application, you must first run the local LLM (gemma3:4b) or component extraction and analysis will not be possible.

